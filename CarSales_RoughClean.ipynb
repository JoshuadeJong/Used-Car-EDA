{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Libraries\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Functions\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Palau': 'PW',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function Reverse Geocoding\n",
    "def getPlace(location):\n",
    "    # Create latList and lonList\n",
    "    latList = df.lat[df.location == location].tolist()\n",
    "    lonList = df.long[df.location == location].tolist()\n",
    "    \n",
    "    # Clean latList, lonList\n",
    "    def cleanList(dirtyList):\n",
    "        index = 0\n",
    "        while(index < len(dirtyList)):\n",
    "            if np.isnan(dirtyList[index]):\n",
    "                del dirtyList[index]\n",
    "            else:\n",
    "                index += 1\n",
    "        return dirtyList\n",
    "    \n",
    "    latList = cleanList(latList)\n",
    "    lonList = cleanList(lonList)\n",
    "    \n",
    "    # Find median lat lon\n",
    "    if 0 != len(latList) or 0 != len(lonList):\n",
    "        lat = np.median(latList)\n",
    "        lon = np.median(lonList)\n",
    "    else:\n",
    "        return['NaN', 'NaN']\n",
    "    \n",
    "    # Reverse Geocode form median lat lon\n",
    "    try:\n",
    "        result = rg.search((lat, lon))\n",
    "    except IndexError:\n",
    "        return [\"ERROR: \" + location, \"ERROR: \" + location]\n",
    "    \n",
    "    # Format output    \n",
    "    return [result[0]['name'], us_state_abbrev[result[0]['admin1']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function finds the spelling mistake with a levenshtein difference of one\n",
    "def fixSpelling(word, possibleWords):\n",
    "    alphabet = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in splits if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "    inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "   \n",
    "    allWords = set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    for element in allWords:\n",
    "        if element in possibleWords:\n",
    "            return element\n",
    "        \n",
    "    return \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Load Data\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('resources/data/CarSale_Data_Raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#  General Cleaning\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all NaN\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Columns\n",
    "df.drop([\"url\", \"city_url\", \"VIN\", \"image_url\", \"desc\"], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "df.rename(columns = {\"make\":\"model\", \"manufacturer\":\"make\", \"city\":\"location\", \"paint_color\":\"color\", \"title_status\":\"title\"}, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Type\n",
    "df = df.astype({'year':int, 'odometer':int});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#  Rough Cuts\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year\n",
    "df = df[(df.year >= 2000) & (df.year != 2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price\n",
    "df = df[df.price <= df.price.quantile(.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odometer\n",
    "#df = df[df.odometer <= df.odometer.quantile(.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce Files Size\n",
    "df = df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Locations\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add City, State Column\n",
    "df.insert(0, \"city\", \"NaN\") # Add new columns for city\n",
    "df.insert(1, \"state\", \"NaN\") # Add new column for state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique locations\n",
    "uniqueLocations = df[\"location\"].unique() \n",
    "\n",
    "# Create and Build dictionary\n",
    "loc2CityState = {}\n",
    "\n",
    "for location in uniqueLocations:\n",
    "    if \",\" in location:    \n",
    "        loc2CityState[location] = location.split(\", \")\n",
    "    elif \"/\" in location:\n",
    "        cityCity = location.split(\"/\")\n",
    "        state = getPlace(location)[1]\n",
    "        loc2CityState[location] = [cityCity[0], state]\n",
    "    elif \"-\" in location:\n",
    "        cityCityCity = location.split(\"-\")\n",
    "        state = getPlace(location)[1]\n",
    "        loc2CityState[location] = [cityCityCity[0], state]\n",
    "    elif location.title() in us_state_abbrev:\n",
    "        city = getPlace(location)[0]\n",
    "        loc2CityState[location] = [city, us_state_abbrev[location.title()]]\n",
    "    else:\n",
    "        state = getPlace(location)[1]\n",
    "        loc2CityState[location] = [location, state]\n",
    "        \n",
    "# Clean location into City State\n",
    "for i in df.index:    \n",
    "    location = df.at[i, 'location']\n",
    "    cityState = loc2CityState[location]\n",
    "    \n",
    "    df.at[i, 'city'] = cityState[0]\n",
    "    df.at[i, \"state\"] = cityState[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop pointless columns\n",
    "df.drop(['location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Makes\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab makes\n",
    "urlmake = \"https://vpic.nhtsa.dot.gov/api/vehicles/GetAllMakes?format=csv\"\n",
    "makeFullList = pd.read_csv(urlmake)['make_name'].tolist()\n",
    "makeFullList = [x.lower() for x in makeFullList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit names\n",
    "df['make'].replace('-', ' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell Check Names\n",
    "makeDict = {}\n",
    "makeList = df['make'].unique().tolist()\n",
    "\n",
    "for word in makeList:\n",
    "    if word in makeFullList:\n",
    "        makeDict[word] = word \n",
    "    else:\n",
    "        makeDict[word] = fixSpelling(word, makeFullList) \n",
    "        \n",
    "df['make'].replace(makeDict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Save\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df.to_csv('resources/data/CarSale_Data_Rough.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
